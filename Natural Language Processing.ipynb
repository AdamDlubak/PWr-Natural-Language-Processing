{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'corpus_ccl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-edb1890e6ab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus2mwe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmwe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmwe_converter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMWEConverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbasicutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_to_coarse_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tools/mwe/mwe_converter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcorpus_ccl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcclutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mccl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpus2mwe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'corpus_ccl'"
     ]
    }
   ],
   "source": [
    "import json, urllib\n",
    "import glob, os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tools import plwn\n",
    "from tools import corpus2\n",
    "from tools import corpus2mwe as mwe\n",
    "from tools.mwe.mwe_converter import MWEConverter\n",
    "\n",
    "from basicutils.simple import convert_to_coarse_pos\n",
    "from corpus2 import AnnotatedSentence_wrap_sentence as annotate_sentence\n",
    "from corpus_ccl import cclutils as ccl\n",
    "from corpus_ccl import corpus_object_utils as cou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_id = 123\n",
    "pd.options.display.max_colwidth = 1000\n",
    "load_data_to_test = False\n",
    "\n",
    "dataset_file = \"Data/czywiesz-eva-I-250.csv\"\n",
    "json_file = 'questions_to_test.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all questions\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    dataset = pd.read_csv(dataset_file, header=None, sep=\";\")\n",
    "    dataset = dataset.drop(columns=[2, 3])\n",
    "    dataset.rename(columns={1: 'Question', 0: 'Question_ID'}, inplace=True)\n",
    "    dataset['Dataset_ID'] = dataset.index\n",
    "    \n",
    "    display(dataset.head(5))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load questions to test\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_test():\n",
    "    with open(json_file) as json_data:  \n",
    "        data = json.load(json_data)\n",
    "    dataset = pd.read_json(data, orient='split')\n",
    "\n",
    "    display(dataset.head(5))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set tagset as NKJP\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagset = ccl.get_tagset('nkjp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_question_to_file(number, filename='question.csv'):\n",
    "    pd.DataFrame(data = { 'Question': [dataset.iloc[1]['Question']]}).to_csv(filename, index=False, header=False, encoding='utf-8')\n",
    "    \n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\",\"<br>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Rest API 2 Functions\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "url=\"http://ws.clarin-pl.eu/nlprest2/base\" \n",
    "\n",
    "def upload(file):    \n",
    "        with open (file, \"r\") as myfile:\n",
    "            doc=myfile.read()\n",
    "        return urllib2.urlopen(urllib2.Request(url+'/upload/',doc,{'Content-Type': 'binary/octet-stream'})).read();\n",
    "\n",
    "def tool(lpmn,user): \n",
    "    data={}\n",
    "    data['lpmn'] = lpmn\n",
    "    data['user'] = user\n",
    "\n",
    "    doc=json.dumps(data)\n",
    "    taskid = urllib2.urlopen(urllib2.Request(url+'/startTask/',doc,{'Content-Type': 'application/json'})).read();\n",
    "    time.sleep(0.1);\n",
    "    resp = urllib2.urlopen(urllib2.Request(url+'/getStatus/'+taskid));\n",
    "    data=json.load(resp)\n",
    "    while data[\"status\"] == \"QUEUE\" or data[\"status\"] == \"PROCESSING\" :\n",
    "        time.sleep(0.1);\n",
    "        resp = urllib2.urlopen(urllib2.Request(url+'/getStatus/'+taskid));\n",
    "        data=json.load(resp)\n",
    "    if data[\"status\"]==\"ERROR\":\n",
    "        print(\"Error \"+data[\"value\"]);\n",
    "        return None   \n",
    "    return data[\"value\"]\n",
    "\n",
    "def tagging(filename = 'question.csv'):\n",
    "    data=upload(filename)\n",
    "\n",
    "    # tutaj następuje złożenie identyfikatora pliku do przetworzenia i ścieżki przetwarzania \n",
    "    data=tool('file('+data+')|any2txt|wcrft2({\"morfeusz2\":false})|liner2({\\\"model\\\":\\\"all\\\"})|wsd({\"use_mwe\":true})','adam.dlubak@gmail.com')\n",
    "    \n",
    "    data=data[0][\"fileID\"];\n",
    "    content = urllib2.urlopen(urllib2.Request(url+'/download'+data)).read();    \n",
    "    with open (os.path.splitext(filename)[0] + '.ccl', \"w\") as outfile:\n",
    "        outfile.write(content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MWEConverter Functions\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(self, ccl_file, out_mwe_file, annots_used=False):\n",
    "    if not self.reader:\n",
    "        self.reader = mwe.CclMWEReader(ccl_file, self.tagset, '/usr/local/share/corpus2mwe/thes-v3.xml')\n",
    "        self.reader.use_annotations(annots_used)\n",
    "    else:\n",
    "        self.reader.set_files(ccl_file)\n",
    "    mwe_doc = self.reader.read()\n",
    "    ccl.write_ccl(mwe_doc, out_mwe_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction Functions\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCase(pos):\n",
    "    posArray = pos.split(\",\")\n",
    "    if(posArray[0] == \"subst\"):\n",
    "        return posArray[1]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_token(sentence, token):\n",
    "    for (index, token_in_sentence) in enumerate(sentence.tokens()):\n",
    "        if token_in_sentence.is_same(token):\n",
    "            return index\n",
    "    raise ValueError(\"Token does not belong to sentence.\")\n",
    "\n",
    "def get_annotations(sentence, token):\n",
    "    \"\"\"\n",
    "    Get annotations of a token from sentence annotation channel.\n",
    "\n",
    "    Args:\n",
    "        sentence (Corpus2.sentence)\n",
    "        token (Corpus2.token)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sentence.all_channels()\n",
    "    except AttributeError:\n",
    "        sentence = annotate_sentence(sentence)\n",
    "\n",
    "    index = _find_token(sentence, token)\n",
    "    # Using dict causes invalid reference, need to retrieve channel anyways\n",
    "    channels = list(sentence.all_channels())\n",
    "    return {name: sentence.get_channel(name).get_segment_at(index)\n",
    "            for name in channels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_data_to_test:\n",
    "    dataset = load_data_to_test()    \n",
    "else:\n",
    "    dataset = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data structures\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = [\"Orth\", \"Base\", \"Ctag\", \"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = pd.DataFrame(data= {'1. Question': \n",
    "     [\"kto\", \"co\", \"który\", \"jaki\", \"kiedy\", \"gdzie\", \"jak\", \"jak często\", \"jak rzadko\", \"którędy\", \"skąd\", \"dokąd\", \"ile\", \"czyje\", \n",
    "      \"czemu\", \"czy\", \"czyj\", \"dlaczego\"], \n",
    "     '2. Description': \n",
    "     [\"Określenie podmiotu / osoby\", \"Określenie rzeczy / zwierzęcia / stanu / pojęć\", \"Określenie jednego spośród wielu\",\n",
    "    \"Określenie cechy elementu\", \"Czas\", \"Miejsce\", \"Sposób\", \"Częstotoliwość\", \"Częstotoliwość\", \"Droga ruchu\", \"Początek ruchu\", \"Cel ruchu\", \"Liczność\",\n",
    "    \"Własność\", \"Powód\", \"Tak/Nie\", \"Przynależność\", \"Powód\"]\n",
    "    })\n",
    "display(table_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working Part\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagger proccess\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_question_to_file(question_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging('question.csv')\n",
    "converter = MWEConverter(tagset='nkjp')\n",
    "converter.convert('question.ccl', 'question-result.ccl')\n",
    "doc = ccl.read_ccl('question-result.ccl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 1 - Extract information about ctag, case and tags\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pos_mask = corpus2.get_attribute_mask(tagset, '')\n",
    "\n",
    "wn = plwn.load_default()\n",
    "nes = defaultdict(set)\n",
    "\n",
    "results = pd.DataFrame(columns = [\"Orth\", \"Base\", \"Ctag\", \"Case\", \"Role\", \"Description\"])\n",
    "double_token = []\n",
    "double_token_description = []\n",
    "time_double_token_description = []\n",
    "time_double_token = []\n",
    "previous_annotation_is_correct = 0\n",
    "previous_time_annotation_is_correct = 0\n",
    "for par in doc.paragraphs():\n",
    "    for sent in par.sentences():\n",
    "        for token in sent.tokens():\n",
    "            annotations = get_annotations(sent, token)\n",
    "            lexeme = token.get_preferred_lexeme(tagset).lemma()\n",
    "            orth = token.orth() \n",
    "            if 'nam' in annotations:\n",
    "                if annotations['nam'] > 0:\n",
    "                    previous_annotation_is_correct = 1\n",
    "                    nes[annotations['nam']].add(token.orth_utf8())\n",
    "                    double_token.append(token.orth_utf8())\n",
    "                    double_token_description = max([k for k,v in annotations.items() if v == 1], key=len)\n",
    "                    results.loc[len(results)] = [orth, \"\", \"\",  \"\", \"\",  \"\"]\n",
    "                        \n",
    "                    continue\n",
    "                elif previous_annotation_is_correct > 0:\n",
    "                    double_token = ' '.join(double_token)\n",
    "                    double_token = double_token.replace(\" .\", \".\")\n",
    "                    double_token = double_token.replace(\" ,\", \",\")\n",
    "                    double_token = double_token.replace(\" :\", \":\")\n",
    "                    \n",
    "                    results.loc[len(results) - 1][\"Base\"] = double_token\n",
    "                    results.loc[len(results) - 1][\"Description\"] = double_token_description\n",
    "                    \n",
    "                    double_token = []\n",
    "                    double_token_description = []\n",
    "                    previous_annotation_is_correct = 0\n",
    "                    \n",
    "            if 'timex' in annotations:\n",
    "                if annotations['timex'] > 0:\n",
    "                    previous_time_annotation_is_correct = 1\n",
    "                    time_double_token.append(token.orth_utf8())\n",
    "                    time_double_token_description = max([k for k,v in annotations.items() if v == 1], key=len)\n",
    "                    results.loc[len(results)] = [orth, \"\", \"\",  \"\", \"\",  \"\"]        \n",
    "                    continue\n",
    "                elif previous_time_annotation_is_correct > 0:\n",
    "                    time_double_token = ' '.join(time_double_token)\n",
    "                    time_double_token = time_double_token.replace(\" .\", \".\")\n",
    "                    time_double_token = time_double_token.replace(\" ,\", \",\")\n",
    "                    time_double_token = time_double_token.replace(\" :\", \":\")\n",
    "                    \n",
    "                    results.loc[len(results) - 1][\"Base\"] = time_double_token\n",
    "                    results.loc[len(results) - 1][\"Description\"] = time_double_token_description\n",
    "                    time_double_token = []\n",
    "                    time_double_token_description = []\n",
    "                    previous_time_annotation_is_correct = 0\n",
    "                    \n",
    "            if 'mwe' in annotations and annotations['mwe'] is 1:\n",
    "                if token.has_metadata():\n",
    "                    md = token.get_metadata()\n",
    "                    if not md.has_attribute('mwe_base'):\n",
    "                        lexeme = \"\"\n",
    "                    else:\n",
    "                        lexeme = md.get_attribute('mwe_base')\n",
    "                else:\n",
    "                    lexeme = \"\"\n",
    "            \n",
    "            if token.has_metadata(): # Jeśli ma <prop>\n",
    "                md = token.get_metadata()\n",
    "                if md.has_attribute('sense:ukb:syns_id'):\n",
    "                    sense = md.get_attribute('sense:ukb:syns_id')\n",
    "                    wn.synset_by_id(sense)\n",
    "            \n",
    "            tag = token.get_preferred_lexeme(tagset).tag()\n",
    "            pos = tagset.tag_to_symbol_string(tag)\n",
    "            \n",
    "\n",
    "            results.loc[len(results)] = [orth, \n",
    "                                         lexeme, \n",
    "                                         convert_to_coarse_pos(pos.split(\",\")[0]),\n",
    "                                         getCase(pos),\n",
    "                                         \"\",\n",
    "                                         \"\"]\n",
    "            t1_pos_mask = tag.get_masked(full_pos_mask)\n",
    "            t1_pos_str = tagset.tag_to_symbol_string(t1_pos_mask)\n",
    "\n",
    "pretty_print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 - Extract information question type\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (str(results['Base'][0]) == \"jak\" or str(results['Base'][0]) == \"Jak\") and (str(results['Base'][1]) == \"często\" or str(results['Base'][1]) == \"rzadko\"):\n",
    "    results.iloc[0, results.columns.get_loc('Role')] = \"Pytanie o Częstotliwość\" + \"\\n\"\n",
    "else:\n",
    "    for idx, row in enumerate(results['Base'].head(3)):\n",
    "        for idx_question, question_row in enumerate(table_df['1. Question']):\n",
    "            if str(row) == str(question_row):\n",
    "                results.iloc[idx, results.columns.get_loc('Role')] = \"Zaimek Pytający\"\n",
    "                results.iloc[idx, results.columns.get_loc('Description')] = \"Pytanie o \" + table_df.iloc[idx_question][\"2. Description\"] + \"\\n\"\n",
    "                \n",
    "pretty_print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 3 - Determine the subject and the predicate\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_base = \"\"\n",
    "\n",
    "for idx, (base, ctag) in enumerate(zip(results[\"Base\"], results[\"Ctag\"])):\n",
    "    if ctag == \"verb\":\n",
    "        if str(base) == \"zostać\" or str(base) == \"być\":\n",
    "            saved_base = str(base)\n",
    "            saved_idx = idx\n",
    "        elif saved_base != \"\":\n",
    "            results.iloc[saved_idx, results.columns.get_loc('Base')] = \"\"\n",
    "            results.iloc[idx, results.columns.get_loc('Base')] = saved_base + \" \" + str(base)\n",
    "            saved_base = \"\"\n",
    "            saved_idx = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (base, case, ctag, role) in enumerate(zip(results[\"Base\"], results[\"Case\"], results[\"Ctag\"], results[\"Role\"])):\n",
    "    if base != \"\" and (ctag == \"verb\"):    \n",
    "        results.iloc[idx, results.columns.get_loc('Role')] = \"Orzeczenie\"   \n",
    "        break\n",
    "        \n",
    "for idx, (base, case, ctag, role) in enumerate(zip(results[\"Base\"], results[\"Case\"], results[\"Ctag\"], results[\"Role\"])):\n",
    "    if base != \"\" and (case == \"nom\" or case == \"gen\") and role == \"\":\n",
    "        results.iloc[idx, results.columns.get_loc('Role')] = \"P\"        \n",
    "    \n",
    "pretty_print(results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = 0\n",
    "for idx, role in enumerate(results[\"Role\"]):\n",
    "    \n",
    "    if role == \"P\" and status == 0:\n",
    "        results.iloc[idx, results.columns.get_loc(\"Role\")] = \"Podmiot\"\n",
    "        status = 1\n",
    "    \n",
    "    elif role == \"P\" and status == 1:\n",
    "        results.iloc[idx, results.columns.get_loc(\"Role\")] = \"\"\n",
    "        \n",
    "        \n",
    "if status == 0:\n",
    "    for idx, desc in enumerate(results[\"Description\"]):\n",
    "        if \"nam\" in desc:\n",
    "            results.iloc[idx, results.columns.get_loc('Role')] = \"P\"    \n",
    "            \n",
    "\n",
    "status = 0\n",
    "for idx, role in enumerate(results[\"Role\"]):\n",
    "    \n",
    "    if role == \"P\" and status == 0:\n",
    "        results.iloc[idx, results.columns.get_loc(\"Role\")] = \"Podmiot\"\n",
    "        status = 1\n",
    "    \n",
    "    elif role == \"P\" and status == 1:\n",
    "        results.iloc[idx, results.columns.get_loc(\"Role\")] = \"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "pretty_print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 4 - Determine meaning of words\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in doc.paragraphs():\n",
    "    for sent in par.sentences():\n",
    "        idx = 0\n",
    "        for token in sent.tokens():\n",
    "            if token.has_metadata():\n",
    "                md = token.get_metadata()\n",
    "                tag = token.get_preferred_lexeme(tagset).tag()\n",
    "                pos = tagset.tag_to_symbol_string(tag)\n",
    "                ctag = results[\"Ctag\"][idx]\n",
    "                if md.has_attribute('sense:ukb:syns_rank') and (ctag == \"noun\" or ctag == \"verb\" or ctag == \"adv\"):\n",
    "                    sense_attribute = md.get_attribute('sense:ukb:syns_rank').split(\" \")\n",
    "                    synset_ids = [item.split(\"/\")[0] for item in sense_attribute]\n",
    "                    synset_probabilities = [round(float(item.split(\"/\")[1]), 2) for item in sense_attribute]\n",
    "\n",
    "                    for (synset_id, synset_probability) in zip(synset_ids, synset_probabilities):\n",
    "                        description = wn.synset_by_id(synset_id).to_dict()['units'][0]['definition']\n",
    "                        variant = wn.synset_by_id(synset_id).to_dict()['units'][0]['variant']\n",
    "                        if description is None or len(description) < 5:\n",
    "                            description = wn.synset_by_id(synset_id).to_dict()['units'][0]['domain']\n",
    "                        synset_desc = str(synset_probability) + \" | \" + \"Wariant: \" + str(variant) + \" | \" + description\n",
    "                        results.iloc[idx, results.columns.get_loc(\"Description\")] = results.iloc[idx, results.columns.get_loc(\"Description\")] + synset_desc + \"\\n\"\n",
    "            idx += 1\n",
    "\n",
    "pretty_print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
